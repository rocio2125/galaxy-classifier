{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dedd650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2321de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total imágenes: 17675\n"
     ]
    }
   ],
   "source": [
    "#   CARGA DE HDF5\n",
    "# ==============================\n",
    "input_h5 = os.path.join(\"data\", \"Galaxy10_DECals_NoDuplicated.h5\") #ruta del archivo .h5 de origen.\n",
    "output_dir = \"reduced_h5\" #carpeta donde se guardarán los HDF5 reducidos.\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "IMG_SIZE = 128\n",
    "\n",
    "h5 = h5py.File(input_h5, \"r\")\n",
    "images = h5[\"images\"]\n",
    "labels = h5[\"ans\"] #obtiene el dataset de etiquetas, cuyo nombre en el HDF5 original es ans.\n",
    "N = images.shape[0]\n",
    "print(\"Total imágenes:\", N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9969e94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splits[train]: 12372, splits['Val']: 2651, splits['Test']: 2652\n"
     ]
    }
   ],
   "source": [
    "#   SPLIT train/val/test\n",
    "# ==============================\n",
    "indices = np.arange(N)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_end = int(0.7 * N) #calculan límites para 70% train, 15% val, 15% test.\n",
    "val_end   = int(0.85 * N)\n",
    "\n",
    "splits = {\n",
    "    \"train\": indices[:train_end],\n",
    "    \"val\":   indices[train_end:val_end],\n",
    "    \"test\":  indices[val_end:]\n",
    "}\n",
    "\n",
    "print(f\"splits[train]: {len(splits['train'])}, splits['Val']: {len(splits['val'])}, splits['Test']: {len(splits['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db338f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCIÓN DE REDIMENSIONADO\n",
    "# =============================\n",
    "def resize_img(img):\n",
    "    return cv2.resize(img, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c4b9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando train: 12372 elementos...\n",
      "0/12372 procesadas...\n",
      "2000/12372 procesadas...\n",
      "4000/12372 procesadas...\n",
      "6000/12372 procesadas...\n",
      "8000/12372 procesadas...\n",
      "10000/12372 procesadas...\n",
      "12000/12372 procesadas...\n",
      "train guardado en: reduced_h5\\train_128.h5\n",
      "Generando val: 2651 elementos...\n",
      "0/2651 procesadas...\n",
      "2000/2651 procesadas...\n",
      "val guardado en: reduced_h5\\val_128.h5\n",
      "Generando test: 2652 elementos...\n",
      "0/2652 procesadas...\n",
      "2000/2652 procesadas...\n",
      "test guardado en: reduced_h5\\test_128.h5\n",
      "\n",
      "Proceso finalizado.\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# CREAR LOS NUEVOS ARCHIVOS H5\n",
    "# =============================\n",
    "for split_name, split_idx in splits.items():\n",
    "    out_path = os.path.join(output_dir, f\"{split_name}_{IMG_SIZE}.h5\") #Para cada split (train, val, test) se crea out_path\n",
    "\n",
    "    file_out = h5py.File(out_path, \"w\")\n",
    "    d_imgs = file_out.create_dataset(\n",
    "        \"images\", \n",
    "        (len(split_idx), IMG_SIZE, IMG_SIZE, 3),\n",
    "        dtype=np.uint8  #crea el dataset con imagenes en 128x128 y type uint8\n",
    "    )\n",
    "    d_lbls = file_out.create_dataset(\n",
    "        \"labels\", \n",
    "        (len(split_idx),),\n",
    "        dtype=np.int32  #crea el dataset de etiquetas.\n",
    "    )\n",
    "\n",
    "    print(f\"Generando {split_name}: {len(split_idx)} elementos...\")\n",
    "\n",
    "    for i, orig_i in enumerate(split_idx):\n",
    "        img = images[orig_i]              # sin cargar a RAM\n",
    "        img_resized = resize_img(img)     # reduce tamaño\n",
    "        d_imgs[i] = img_resized\n",
    "        d_lbls[i] = labels[orig_i]\n",
    "\n",
    "        if i % 2000 == 0: #imprimimos progreso cada 2000 imagenes procesadas\n",
    "            print(f\"{i}/{len(split_idx)} procesadas...\")\n",
    "\n",
    "    file_out.close()\n",
    "    print(f\"{split_name} guardado en: {out_path}\")\n",
    "\n",
    "h5.close()\n",
    "print(\"\\nProceso finalizado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
